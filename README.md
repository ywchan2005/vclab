# References

### Neural Network 101
- LeCun, Yann A., et al. "[Efficient backprop.](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)" *Neural networks: Tricks of the trade*. Springer Berlin Heidelberg, 2012. 9–48.

### Universal Approximability
- Cybenko, George. "[Approximation by superpositions of a sigmoidal function.](https://www.dartmouth.edu/~gvc/Cybenko_MCSS.pdf)" *Mathematics of Control, Signals, and Systems (MCSS)* 2.4 (1989): 303-314.
- Hornik, Kurt. "[Approximation capabilities of multilayer feedforward networks.](http://zmjones.com/static/statistical-learning/hornik-nn-1991.pdf)" *Neural networks* 4.2 (1991): 251-257.
- Sonoda, Sho, and Noboru Murata. "[Neural network with unbounded activation functions is universal approximator.](https://arxiv.org/abs/1505.03654)" *Applied and Computational Harmonic Analysis* (2015).

### Activation Function
- Bengio, Yoshua, Patrice Simard, and Paolo Frasconi. "[Learning long-term dependencies with gradient descent is difficult.](http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf)" *IEEE transactions on neural networks* 5.2 (1994): 157-166.
- Glorot, Xavier, Antoine Bordes, and Yoshua Bengio. "[Deep Sparse Rectifier Neural Networks.](http://www.jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf)" *International Conference on Artificial Intelligence and Statistics*. 2011.
- Goodfellow, Ian, et al. "[Maxout Networks.](https://arxiv.org/abs/1302.4389)" *Proceedings of the 30th International Conference on Machine Learning (ICML-13)*. 2013.
- Springenberg, Jost Tobias, and Martin Riedmiller. "[Improving deep neural networks with probabilistic maxout units.](https://arxiv.org/abs/1312.6116)" *arXiv preprint arXiv:1312.6116* (2013).
- Lin, Min, Qiang Chen, and Shuicheng Yan. "[Network in network.](https://arxiv.org/abs/1312.4400)" *Proceedings of the 2nd International Conference on Learning Representations*. (2014).
- Clevert, Djork-Arné, Thomas Unterthiner, and Sepp Hochreiter. "[Fast and accurate deep network learning by exponential linear units (elus).](https://arxiv.org/abs/1511.07289)" *arXiv preprint arXiv:1511.07289* (2015).
- Gulcehre, Caglar, et al. "[Noisy Activation Functions.](http://www.jmlr.org/proceedings/papers/v48/gulcehre16.pdf)" *Proceedings of The 33rd International Conference on Machine Learning*. 2016.

### Vanishing Gradient Problem
- Pascanu, Razvan, Tomas Mikolov, and Yoshua Bengio. "[On the difficulty of training recurrent neural networks.](https://arxiv.org/abs/1211.5063)" *Proceedings of The 30th International Conference on Machine Learning*. 2013.

### Oversampling
- Chawla, Nitesh V., et al. "[SMOTE: synthetic minority over-sampling technique.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.86.6835&rep=rep1&type=pdf)" *Journal of artificial intelligence research* 16 (2002): 321–357.
- Nguyen, Hien M., Eric W. Cooper, and Katsuari Kamei. "[Borderline over-sampling for imbalanced data classification.](http://ousar.lib.okayama-u.ac.jp/files/public/1/19617/20160528004522391723/IWCIA2009_A1005.pdf)" *International Journal of Knowledge Engineering and Soft Data Paradigms* 3.1 (2011): 4–21.
