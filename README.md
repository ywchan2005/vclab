# References

### Neural Network 101
- LeCun, Yann A., et al. “[Efficient backprop.](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)” *Neural networks: Tricks of the trade*. Springer Berlin Heidelberg, 2012. 9–48.

### Activation Function
- Bengio, Yoshua, Patrice Simard, and Paolo Frasconi. "[Learning long-term dependencies with gradient descent is difficult.](http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf)" *IEEE transactions on neural networks* 5.2 (1994): 157-166.
- Gulcehre, Caglar, et al. "[Noisy Activation Functions.](http://www.jmlr.org/proceedings/papers/v48/gulcehre16.pdf)" *Proceedings of The 33rd International Conference on Machine Learning*. 2016.

### Oversampling
- Nguyen, Hien M., Eric W. Cooper, and Katsuari Kamei. “[Borderline over-sampling for imbalanced data classification.](http://ousar.lib.okayama-u.ac.jp/files/public/1/19617/20160528004522391723/IWCIA2009_A1005.pdf)” *International Journal of Knowledge Engineering and Soft Data Paradigms* 3.1 (2011): 4–21.
- Chawla, Nitesh V., et al. “[SMOTE: synthetic minority over-sampling technique.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.86.6835&rep=rep1&type=pdf)” *Journal of artificial intelligence research* 16 (2002): 321–357.
